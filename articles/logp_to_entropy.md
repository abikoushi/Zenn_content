---
title: "確率の対数を取る（あるいは情報エントロピー入門）"
emoji: "🎉"
type: "tech" # tech: 技術記事 / idea: アイデア
topics: [Julia]
published: false
---

## 前置き

エントロピーは情報理論や物理学でも出てくるが，このノートのモチベーションは統計学の立場から．しかし，表記などは異なるが基本的にはどの分野でもエントロピーと呼ばれるものはおなじものと思われる．

前半のエントロピーの話は一応，要求される予備知識は高校数学の範囲内におさめたつもりだがどうだろうか．後半は大数の法則とかちょっと大学レベルの内容も出てくる．

数値例は Julia 言語による．

## エントロピー

確率の話でよく一番基本的な例として出てくる「公平なコイン投げ」を考える．表が出る確率は $1/2$ である． $k$ 回のコイン投げで $k$ 回表が出る確率は $2^{-k}$ だ．

コイン投げの例がわかりやすいとしたら，逆に確率 $p$ をコイン投げでいうと何回表が出る程度の珍しさかで比べるという発想もありうる．

2 を $-k$ 乗したら $p$ になるような数 $k$ （ $2^{-k} = p$ となる $k$ ）は，対数の定義から次のように表せる．

$$
k = -\log_2 p .
$$

右辺を情報量と呼ぶことにし，情報量の期待値をエントロピーと呼ぶ．

公平とは限らないコイン投げを考え，確率 $p$ で表， $1-p$ で裏が出るとすると，情報量の期待値 $H(p)$ は次の式で表せる．

$$
H(p) = - ( p\log_2 p + (1-p)\log_2 (1-p)) \tag{1}.
$$

期待値の定義を復習しておく．期待値は取りうる値の範囲全体にわたって，（確率）×（取りうる値）を足し合わせたものである．コイン投げの場合，取りうる値とその確率は次の表のように書き下せる．

||裏|表|
|--|--|--|
|確率|$p$|$1-p$|
|情報量|$-\log_2 p$|$-\log_2 (1-p)$|

取りうる値の範囲に対して確率がすべて書き下せるものを **確率分布** と呼ぶ．また，期待値が計算できるよう，確率分布と組で定義された変数を **確率変数** と呼ぶことにする．公平とは限らないコイン投げの場合のこの確率分布を2項分布と呼び，(1)の $H(p)$ は2項分布のエントロピーである．

取りうる値が裏・表の2通りでなくても，エントロピーは同様に考えることができる．公平とは限らない $m$ 面サイコロ，つまり状態 $i=1,\ldots,m$ がそれぞれ $p_i$ の確率で生起する分布を考える．この場合エントロピー $H(p)$ は次のようになる．

$$
H(p) = - \sum_{i=1}^{m} p_i \log_2 p_i \tag{2}.
$$

期待値を計算できるように確率分布をすべて書き下すと次の表のようになる．

||1|2|$\ldots$|$m$|
|--|--|--|--|--|
|確率| $p_1$ | $p_2$ | $\ldots$ |$p_m$|
|情報量|$-\log_2 p_1$|$-\log_2 p_2$|$\ldots$|$-\log_2 p_m$|

この公平とは限らない $m$ 面サイコロの確率分布を多項分布と呼び，(2)の $H(p)$ は多項分布のエントロピーである．

対数の底は2にすることも多いが，ネイピア数 $e$ にすることも多い．底の変換は次のように定数倍で得られる．

$$
\log_2 p = \log_e p / \log_e 2
$$

相対的比較だけのためなら対数の底は割となんでもいいということである．割となんでもいいが 2 にしておくと感覚的にはわかりやすい．物理量と対応させるには $e$ にしておくと都合がいいらしい．ちなみに $\log_e 2$ は 0.6931472 くらい，$1/e$ は 0.3678794 くらいである．以降では $\log_e p = \log p$ と省略して書く．

### 数値例

Julia の Distributions.jl パッケージでエントロピーを計算してみる．
まず，下の図のような公平なサイコロの分布について求める．

![](/images/logp_to_entropy/bar_dice0.png)

次のようにする．

```julia
using Distributions
using Plots
using LogExpFunctions

p0 = repeat([1/6], 6)

dice0 = Categorical(p0)
entropy(dice0)
```

確率分布自体をオブジェクト `dice0` として持てて，確率分布のエントロピーを求める関数 `entropy` があらかじめ定義されているのがよいところだ．値は約 1.79 だった．

```
julia> entropy(dice0)
1.7917594692280547
```

このエントロピーは底が $e$ の対数（自然対数）で，次のように計算したものと一致することがわかる．

```julia
-sum(xlogx, p0)
```

次に，下の図のようなゆがんだサイコロの分布についてエントロピーを求める．

![](/images/logp_to_entropy/bar_dice1.png)

```julia
dice1 = Categorical( [0.1, 0.05, 0.1, 0.2, 0.15, 0.4] )
entropy(dice1)
```

値は約 1.58 だった．

```
julia> entropy(dice1)
1.5832755052458731
```

ゆがんだサイコロの分布のほうがエントロピーが小さい．
エントロピーは「乱雑さの指標」のように紹介されることがあり，人によってはゆがんだサイコロのほうが（値がそろってないので）乱雑に感じられるかもしれない．この場合の「乱雑さ」は（エネルギー，または適当なギャンブルを想像して）「利用しにくさ」のように捉えるとよい．確率に偏りがあればそこに賭けて勝ちやすくすることもできるかもしれないが，フラットなときはランダムに賭けるより勝ちやすくはできない．

次のような，完全にイカサマのサイコロについてはエントロピーが 0 になる．


![](/images/logp_to_entropy/bar_dice2.png)

```julia
dice2 = Categorical( [1,0,0,0,0,0] )
entropy(dice2)
```

```
julia> entropy(dice2)
-0.0
```

このような場合のため，確率関連の数値計算を実装する際には　$0 \log 0 = 0$ としておくと便利である．


## 相対エントロピー

状態 $i=1,\ldots,m$ がそれぞれ $p_i$ の確率で生起する多項分布を $p$ と書くことにする．2つの多項分布 $p$ と $q$ の相対エントロピー $B(p, q)$ は，

$$
B(p, q)=- \sum_{i=1}^{m} p_i \log {\frac {p_i}{q_i}}
$$

と定義される．少しだけ式が長くなるが，次のように書いたほうがエントロピーとの対応がわかりやすいかもしれない．

$$
B(p, q)=- \sum_{i=1}^{m} q_i \frac{p_i}{q_i} \log {\frac {p_i}{q_i}}.
$$

$p_i/q_i$ という比を考えていることが「相対」っぽさである．

また，次のような形にも書き換えてみる．

$$
B(p, q)=\left( -\sum_{i=1}^{m} p_i \log {q_i} \right) - \left(-\sum_{i=1}^{m} p_i \log p_i\right) \tag{3}
$$

(3) の第2項は（2）の式で出てきた $p$ の分布のエントロピー $H(p)$ である．$p$ の分布のエントロピーをベースラインとして引き算して，そこからの離れ具合を考えていることが「相対」っぽさである．

最後に，重要なこととして相対エントロピーは真の分布が $p$ のとき，観測値がほぼ $q$ となる確率の対数と解釈できる．どういうことかを説明する．

多項分布に従って，確率変数を $n$ 回観測することを考えよう．状態 $i=1,\ldots,m$ が生起した回数をそれぞれ $n_1,\ldots,n_m$ とする． $n=\sum_{i=1}^{m}n_i$ である．

このとき，$p$ の分布を知らない人が，確率分布はおおよそ $q$ の分布ではないかと推測する場面を思い浮かべてほしい． 現実に自然や社会についてデータを取るとき，データの確率分布は不明で（それがわかっていたらデータを集める必要がない）いろいろ実験とか調査をして推測するので，これは現実の問題と直接の関係を持つシチュエーションである．設定した $q$ の分布から $n_1, \ldots, n_m$ のような観測が得られる確率 $L(q)$ （これを尤度と呼ぶ）は，次の式で表せる．

$$
L(q)=\frac{n!}{n_1! \cdots n_m!}q^{n_1}_1 \cdots q^{n_m}_m.
$$

ここでスターリングの公式 $\log (n!) \approx n \log n -n$ を使って $\log L(q)$ を近似すると，

$$
\begin{aligned}
\log L(q) & \approx (n \log n) - \sum_{i=1}^{m}( n_i \log n_i -n_i) + \sum_{i=1}^{m} \log q_i \\
&= n \log n - \sum_{i=1}^m n_i (\log n_i - q_i) \\
&= - \sum_{i=1}^m n_i (\log n_i - q_i -\log n ) \\
&= - \sum_{i=1}^m n_i (\log \frac{n_i}{n} - q_i) \\
&= - n \sum_{i=1}^m \frac{n_i}{n} (\log \frac{n_i}{n} - q_i) 
\end{aligned}
$$

と整理できる．大数の法則により，$n$ が大きいとき $n_i/n$ は $p_i$ に近づくはずだから，上の式で $n_i/n =p_i$ と置くと，

$$
\log L(q) \approx -n\sum_{i=1}^{m}p_i \log \frac{p_i}{q_i} = n B(p, q)
$$

という結果を得る．左辺はもともと手元にある観測値の分布（経験分布）がモデル $q$ のように振る舞う確率の対数であったが，右辺に相対エントロピーが得られた．

相対エントロピーの符号反転（$-1$をかけたもの）はカルバック・ライブラ情報量とも呼ばれる．カルバック・ライブラ情報量は今日の統計学ではモデルの良さ（悪さ）を測る基本的な指標になっている．マイナスがついているのでカルバック・ライブラ情報量が大きいほど確率が小さい，すなわち $q$ が $p$ から遠いということを意味する．統計学の分野で「情報量規準」と呼ばれる AIC (An Information Criterion または Akaike Information Criterion) や WAIC (Widely Applicable Information Criterion または Watanabe-Akaike information criterion) は (3) の第1項をサンプルから近似するものである．

### 数値例

2つの確率分布のカルバック・ライブラ情報量を求める関数 `kldivergence` もあらかじめ用意されている．

$p$ を公平なコイン投げに固定し， $q$ のほうを動かしてプロットしてみる．

```julia
coin0 = Bernoulli(0.5)
p = plot(x -> kldivergence(coin0, Bernoulli(x)), legend=false)
xlabel!(p, "q")
ylabel!(p, "KLD")
```

![](/images/logp_to_entropy/line_kld.png)

次に $p$ と $q$ の両方を動かしてプロットしてみる．

```julia
f(x, y) = kldivergence(Bernoulli(x), Bernoulli(y))
x = range(0.01, 0.99, length=100)
y = range(0.01, 0.99, length=100)
z = @. f(x', y)
p = heatmap(x, y, z)
xlabel!(p, "p")
ylabel!(p, "q")
```

![](/images/logp_to_entropy/heat_kld.png)

等高線が楕円状になっていることからわかるように，カルバック・ライブラ情報量は左右対称ではない．つまり $B(p,q) = B(q,p)$ は一般には成り立たない．しかし，$p$ が真の分布で $q$ がその近似のように考えるときは対称でなければいけないということはないし，「経験分布がモデル $q$ のように振る舞う確率がおおよそ $\exp(nB(p, q))$ 」のようなわかりやすい性質が成り立つことが大事である．一方で，常にカルバック・ライブラ情報量を使うといいかというとそんなこともなく，例えば二項分布の正規分布近似のようなことを考えるときはある点に確率 0 が来てしまい，いつまでも無限に遠いことになる．

Julia のコード全体は以下にまとめた．



## 参考にした文献など

前半のエントロピーについては主に [都筑卓司『マックスウェルの悪魔』（講談社ブルーバックス）](https://www.kodansha.co.jp/book/products/0000194345) のⅥ章を参考にした．似た解説は [田中章詞，富谷昭夫，橋本幸士『ディープラーニングと物理学』（講談社）](https://www.kodansha.co.jp/book/products/0000318303) の第1章にもある．しかしこれらの本の物理の話はこのノートではまったく触れられなかった．


後半の相対エントロピーについては主に， [赤池弘次『エントロピーとモデルの尤度』（日本物理学会誌）](https://www.jstage.jst.go.jp/article/butsuri1946/35/7/35_7_608/_article/-char/ja/) を参考にした．似た解説は [島谷健一郎『ポアソン分布・ポアソン回帰・ポアソン過程』（近代科学社）](https://www.kodansha.co.jp/book/products/0000318303) の第4章にもある．


最後のほうで少しだけ触れたカルバック・ライブラ情報量と統計学の情報量規準については [渡辺澄夫『ベイズ統計の理論と方法』（コロナ社）](https://www.coronasha.co.jp/np/isbn/9784339024623/) に詳しい．ベイズ統計の自由エネルギーも同様にカルバック・ライブラ情報量に基づく評価として使うことができる．BIC (Bayesian Information Criterion) や WBIC (Widely applicable Bayesian Information Criterion) は自由エネルギーの近似であるが，WAIC が事後（posterior）予測分布の良さを評価しているのに対し自由エネルギーは事前（prior）予測分布の良さを見ている．これについては [自由エネルギーもカルバック・ライブラー情報量に基づく情報量規準とみなせる](https://selfboast.hatenablog.jp/entry/2021/06/14/040322) に少し書いた．


ここで紹介した本はそれぞれ目指すところは異なるが，どれもそれぞれ個性的なので気になったものは読んでみてほしい．[赤池弘次『エントロピーとモデルの尤度』（日本物理学会誌）](https://www.jstage.jst.go.jp/article/butsuri1946/35/7/35_7_608/_article/-char/ja/) だけは本でなく一つの論文だが，無料だし短いので読みやすく，特におすすめする．
